{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from re import sub\n",
    "from pandas import DataFrame\n",
    "from pymystem3 import Mystem\n",
    "import texterra\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from itertools import chain\n",
    "from pickle import dump, load\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import SpectralClustering, AgglomerativeClustering, KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import *\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "API_KEY = '9988cfb979b80264baeba1386cc7e455f99f943c'\n",
    "\n",
    "stop = stopwords.words('russian')\n",
    "la = np.linalg\n",
    "morph = MorphAnalyzer()\n",
    "m = Mystem()\n",
    "# t = texterra.API(API_KEY)\n",
    "alpha_tokenizer = RegexpTokenizer('[A-Za-zА-Яа-я]\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame, concat\n",
    "from os import path\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "# import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 1 [10 баллов] \n",
    "# До 1.12.17 23:59\n",
    "\n",
    "Задание выполняется в группе (1-4 человека). В случае использования какого-либо строннего источника информации обязательно дайте на него ссылку (поскольку другие тоже могут на него наткнуться). Плагиат наказывается нулём баллов за задание и предвзятым отношением в будущем.\n",
    "\n",
    "Не все части обязательны для выполнения, однако вы можете быть дополнительно оштрафованы за небрежное за выполнение одной или двух частей вместо четырех.\n",
    "\n",
    "При возниконовении проблем с выполнением задания обращайтесь с вопросами к преподавателю. Поэтому настоятельно рекомендуется выполнять задание заранее, оставив запас времени на всевозможные технические проблемы. Если вы начали читать условие в последний вечер и не успели из-за проблем с установкой какой-либо библиотеки — это ваши проблемы.\n",
    "\n",
    "\n",
    "Результат выполнения задания — это отчёт в формате html на основе Jupyter Notebook. Нормальный отчёт должен включать в себя:\n",
    "* Краткую постановку задачи и формулировку задания\n",
    "* Описание **минимума** необходимой теории и/или описание используемых инструментов - не стоит переписывать лекции или Википедию\n",
    "* Подробный пошаговый рассказ о проделанной работе\n",
    "* Аккуратно оформленные результаты\n",
    "* **Внятные выводы** – не стоит относится к домашнему заданию как к последовательности сугубо технических шагов, а стоит относится скорее как к небольшому практическому исследованию, у которого есть своя цель и свое назначение.\n",
    "\n",
    "Небрежное его оформление отчета существенно отразится на итоговой оценке. Весь код из отчёта должен быть воспроизводимым, если для этого нужны какие-то дополнительные действия, установленные модули и т.п. — всё это должно быть прописано в тексте в явном виде.\n",
    "\n",
    "Сдача отчетов осуществляется через систему AnyTask.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация новостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входная коллекция данных состоит из двух частей:\n",
    "1. events.csv – список 28 резонансных событий первой половины 2017 года, каждому событию присвоен свой порядковый номер (id)\n",
    "2. raw_news.csv – тексты новостей из различных новостных источников, известно, к какому резонансному событию относится каждая новость (столбец event_id).\n",
    "\n",
    "Будем считать, что одно событие – это один кластер. В этом домашнем задании вам предстоит:\n",
    "1. провести кластеризацию текстов новостей и проверить, получается ли восстановить кластерную структуру \n",
    "2. проверить, можно ли использовать кластерный анализ для обобщения: найти небольшое число кластеров и проверить, получается ли выделить общие направлени новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_events = DataFrame.from_csv('events.csv')\n",
    "df_news = DataFrame.from_csv('raw_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>В ПЕТЕРБУРГЕ ПРОШЕЛ МИТИНГ ПРОТИВ ПЕРЕДАЧИ ИС...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lenta.co, Москва, 14 января 2017 СИТУАЦИЯ С П...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Аргументы и Факты (aif.ru), Москва, 14 января...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Google Новости ТОП, Москва, 14 января 2017 АК...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Газета.Ru, Москва, 13 января 2017 В МОСКОВСКО...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id                                               text\n",
       "0         1   В ПЕТЕРБУРГЕ ПРОШЕЛ МИТИНГ ПРОТИВ ПЕРЕДАЧИ ИС...\n",
       "1         1   Lenta.co, Москва, 14 января 2017 СИТУАЦИЯ С П...\n",
       "2         1   Аргументы и Факты (aif.ru), Москва, 14 января...\n",
       "3         1   Google Новости ТОП, Москва, 14 января 2017 АК...\n",
       "4         1   Газета.Ru, Москва, 13 января 2017 В МОСКОВСКО..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1 [2 балла] Предварительная обработка текстов\n",
    "Проведите предобработку новостей: токенизацию, приведение к нижнему регистру, лемматизацию. Проверьте, есть ли в коллекции дубликаты. Посчитайте, сколько новостей относится к каждому резонансному событию. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Для начала смерджим все тексты новостей для того, чтобы получить один единый корпус, с которым можно работать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = ' '.join(df_news.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистка новостей от шумов (пока реализовано только удаление УРЛОВ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return sub(r'http\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = remove_url(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация (удаляем всё, что не является alphanumeric-символами):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = alpha_tokenizer.tokenize(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация (и автоматические приведение к нижнему регистру). Мы решили рассмотреть три варианта лемматизации, поскольку не все способны одинаково хорошо справляться с контекстной омонимией (как, например, во фразе \"Запотело стекло. Варенье стекло по краю банки.\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# norm_tokens = [morph.parse(word)[0].normal_form for word in tokens]\n",
    "\n",
    "with open('lemmas_from_texterra.pickle', 'rb') as f:\n",
    "    texts = load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сколько новостей относится к каждому резонансному событию?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17    102\n",
       "28    100\n",
       "18    100\n",
       "7     100\n",
       "10    100\n",
       "12    100\n",
       "27    100\n",
       "16    100\n",
       "1     100\n",
       "25    100\n",
       "26    100\n",
       "21    100\n",
       "23    100\n",
       "3      84\n",
       "22     82\n",
       "9      82\n",
       "24     62\n",
       "4      62\n",
       "2      51\n",
       "11     49\n",
       "19     45\n",
       "6      41\n",
       "8      27\n",
       "13     24\n",
       "20      8\n",
       "15      7\n",
       "5       2\n",
       "14      2\n",
       "Name: event_id, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.event_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ли в коллекции дубликаты?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     event_id                                               text\n",
       "513         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "518         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "522         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "526         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "528         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "536         9   Коммерсантъ. Новости информ. центра, Москва, ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news[df_news.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2 [3 балла] Кластеризация текстов\n",
    "Любым известным вам алгоритмом найдите в коллекции новостей 28 кластеров. Используйте $tf-idf$ преобразование, сингулярное разложение, нормировку признакого пространства и любые другие техники обработки данных, которые вы считаете нужным. Используйте любые известные вам меры качества для оценки качества кластеризации. Покажите, как  $tf-idf$ преобразование или сингулярное разложение или любая другая использованная вами техника влияет на качество кластеризации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем Truncated SVD (a.k.a Latent Semantic Analysis):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = [' '.join([word for word in sentence]) for sentence in texts]\n",
    "ngram_range=(1,1)\n",
    "n_clusters=28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectroizer: TF-IDF Vectorizer, Clusterizer: AgglomerativeClustering \n",
      "ARI = 0.46, V = 0.75, completenss = 0.74, homogenity = 0.74 \n",
      "======\n"
     ]
    }
   ],
   "source": [
    "for vectorizer, vectorizer_name in [\n",
    "                        (TfidfVectorizer(ngram_range=ngram_range), 'TF-IDF Vectorizer'),\n",
    "                          #(CountVectorizer(ngram_range=ngram_range), 'Count Vectorizer'), \n",
    "                          #(HashingVectorizer(ngram_range=ngram_range), 'Hashing Vectorizer')\n",
    "                        ]:\n",
    "    count_model = vectorizer\n",
    "    X = count_model.fit_transform(words_list)\n",
    "    svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "    U = svd.fit_transform(X)  \n",
    "    for clusterizer, clusterizer_name in [\n",
    "                                        #(SpectralClustering(n_clusters=n_clusters), 'Spectral Clustering'),\n",
    "                                         #(KMeans(n_clusters=n_clusters), 'KMeans'),\n",
    "                                         (AgglomerativeClustering(n_clusters=n_clusters), 'AgglomerativeClustering')\n",
    "                                        ]:\n",
    "        clusterizer_model = clusterizer.fit(U)\n",
    "        print('Vectroizer: {}, Clusterizer: {} \\nARI = {:0.2f}, V = {:0.2f}, completenss = {:0.2f}, homogenity = {:0.2f} \\n======'.format(\n",
    "                    vectorizer_name,\n",
    "                    clusterizer_name,\n",
    "                    adjusted_rand_score(clusterizer_model.labels_, df_news.event_id),\n",
    "                    completeness_score(clusterizer_model.labels_, df_news.event_id),\n",
    "                    v_measure_score(clusterizer_model.labels_, df_news.event_id),\n",
    "                    homogeneity_score(clusterizer_model.labels_, df_news.event_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем Normalizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectroizer: TF-IDF Vectorizer, Clusterizer: Spectral Clustering \n",
      "ARI = 0.80, V = 0.91, completenss = 0.90, homogenity = 0.89 \n",
      "======\n",
      "Vectroizer: TF-IDF Vectorizer, Clusterizer: KMeans \n",
      "ARI = 0.80, V = 0.91, completenss = 0.90, homogenity = 0.89 \n",
      "======\n",
      "Vectroizer: TF-IDF Vectorizer, Clusterizer: AgglomerativeClustering \n",
      "ARI = 0.80, V = 0.91, completenss = 0.90, homogenity = 0.89 \n",
      "======\n",
      "Vectroizer: Count Vectorizer, Clusterizer: Spectral Clustering \n",
      "ARI = 0.80, V = 0.91, completenss = 0.90, homogenity = 0.89 \n",
      "======\n",
      "Vectroizer: Count Vectorizer, Clusterizer: KMeans \n",
      "ARI = 0.80, V = 0.91, completenss = 0.90, homogenity = 0.89 \n",
      "======\n",
      "Vectroizer: Count Vectorizer, Clusterizer: AgglomerativeClustering \n",
      "ARI = 0.80, V = 0.91, completenss = 0.90, homogenity = 0.89 \n",
      "======\n",
      "Vectroizer: Hashing Vectorizer, Clusterizer: Spectral Clustering \n",
      "ARI = 0.80, V = 0.91, completenss = 0.90, homogenity = 0.89 \n",
      "======\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-5ec829a73a00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                                          \u001b[0;34m(\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KMeans'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                          (AgglomerativeClustering(n_clusters=n_clusters), 'AgglomerativeClustering')]:\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mclusterizer_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusterizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         print('Vectroizer: {}, Clusterizer: {} \\nARI = {:0.2f}, V = {:0.2f}, completenss = {:0.2f}, homogenity = {:0.2f} \\n======'.format(\n\u001b[1;32m     17\u001b[0m                     \u001b[0mvectorizer_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/defeater/anaconda/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/defeater/anaconda/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 x_squared_norms=x_squared_norms, random_state=random_state)\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/defeater/anaconda/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[0;34m(X, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initialization complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     centers, labels, n_iter = k_means_elkan(X, n_clusters, centers, tol=tol,\n\u001b[0;32m--> 399\u001b[0;31m                                             max_iter=max_iter, verbose=verbose)\n\u001b[0m\u001b[1;32m    400\u001b[0m     \u001b[0minertia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minertia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/cluster/_k_means_elkan.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means_elkan.k_means_elkan (sklearn/cluster/_k_means_elkan.c:7266)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/defeater/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1710\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m     \"\"\"\n\u001b[1;32m   1712\u001b[0m     \u001b[0mSum\u001b[0m \u001b[0mof\u001b[0m \u001b[0marray\u001b[0m \u001b[0melements\u001b[0m \u001b[0mover\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for vectorizer, vectorizer_name in [(TfidfVectorizer(ngram_range=ngram_range), 'TF-IDF Vectorizer'),\n",
    "                          (CountVectorizer(ngram_range=ngram_range), 'Count Vectorizer'), \n",
    "                          (HashingVectorizer(ngram_range=ngram_range), 'Hashing Vectorizer')]:\n",
    "    count_model = vectorizer\n",
    "    X = count_model.fit_transform(words_list)\n",
    "    normalizer = Normalizer()\n",
    "    U = normalizer.fit_transform(X)\n",
    "    for clusterizer, clusterizer_name in [(SpectralClustering(n_clusters=n_clusters), 'Spectral Clustering'),\n",
    "                                         (KMeans(n_clusters=n_clusters), 'KMeans'),\n",
    "                                         (AgglomerativeClustering(n_clusters=n_clusters), 'AgglomerativeClustering')]:\n",
    "        clusterizer_model = clusterizer.fit(U.toarray())\n",
    "        print('Vectroizer: {}, Clusterizer: {} \\nARI = {:0.2f}, V = {:0.2f}, completenss = {:0.2f}, homogenity = {:0.2f} \\n======'.format(\n",
    "                    vectorizer_name,\n",
    "                    clusterizer_name,\n",
    "                    adjusted_rand_score(spectral.labels_, df_news.event_id),\n",
    "                    completeness_score(spectral.labels_, df_news.event_id),\n",
    "                    v_measure_score(spectral.labels_, df_news.event_id),\n",
    "                    homogeneity_score(spectral.labels_, df_news.event_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3 [5 баллов] Кластеризация текстов (продолжение)\n",
    "Задайте число кластеров заведомо меньше 28 (например, 5 кластеров). Повторно кластеризуйте новости. Можете ли вы проинтепретировать полученные кластеры? Получается ли с помощью кластерного анализа определить в каких сферах жизни (например, культура, политика, спорт) произошли события?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "X = vectorizer.fit_transform(words_list)\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "U = svd.fit_transform(X) \n",
    "clusterizer = SpectralClustering(n_clusters=5)\n",
    "clusterizer_model = clusterizer.fit(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clusterizer_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_merged = []\n",
    "\n",
    "for event in range(5):\n",
    "    events = set([df_news.event_id.values[i] for i, x in enumerate(labels) if x == event])\n",
    "    events_merged.append(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дональд Трамп вступил в должность президента США.\n",
      "CNN показала фильм «Владимир Путин — самый влиятельный человек в мире».\n",
      "Юлию Самойлову не пустили на евровидении в Киеве\n",
      "В центре Киева был убит бывший депутат Госдумы РФ от КПРФ Денис Вороненков\n",
      "Тиллерсон посещает Москву и встречается с Путиным\n",
      "Несанкционированные акции в Москве апрель\n",
      "Путин и Меркель в Сочи\n",
      "Победа Макрона во Франции\n",
      "Горячая линия Президента Путина\n",
      "Саммит G20\n",
      "Единый день голосования\n",
      "=======\n",
      "Дональд Трамп вступил в должность президента США.\n",
      "Скоропостижно скончался постпред России при ООН Виталий Чуркин.\n",
      "Вышел фильм Навального «он Вам не димон»\n",
      "Умер Дэвид рокфеллер\n",
      "теракт произошел в центре Лондона\n",
      "Юлию Самойлову не пустили на евровидении в Киеве\n",
      "В центре Киева был убит бывший депутат Госдумы РФ от КПРФ Денис Вороненков\n",
      "Митинг в москве против коррупции\n",
      "SpaceX впервые в истории запустила и посадила уже летавшую ракету-носитель\n",
      "Умер Евгений Евтушенко\n",
      "Премьер Медведев выступает перед депутатами Госдумы с отчетом об итогах работы правительства за 2016 год\n",
      "Несанкционированные акции в Москве апрель\n",
      "Чемпионат мира по хоккею\n",
      "Победа Макрона во Франции\n",
      "Митинг против Реновации в Москве\n",
      "Ураган в Москве\n",
      "Парламентские выборы в Великобритании\n",
      "Горячая линия Президента Путина\n",
      "Кубок конфедерации FiFA\n",
      "Теракт в Барселоне\n",
      "Единый день голосования\n",
      "=======\n",
      "Власти Петербурга согласились передать РПЦ Исаакиевский собор.\n",
      "Вышел фильм Навального «он Вам не димон»\n",
      "Митинг в москве против коррупции\n",
      "Несанкционированные акции в Москве апрель\n",
      "Митинг против Реновации в Москве\n",
      "Акции протеста 12 июня\n",
      "Кубок конфедерации FiFA\n",
      "=======\n",
      "Скоропостижно скончался постпред России при ООН Виталий Чуркин.\n",
      "Умер Евгений Евтушенко\n",
      "Умер Дэвид рокфеллер\n",
      "=======\n",
      "Правительство внесло в Госдуму законопроект о курортных сборах\n",
      "=======\n"
     ]
    }
   ],
   "source": [
    "for events_set in events_merged:\n",
    "    for event in events_set:\n",
    "        print(df_events.loc[df_events.index == event]['name'].values[0])\n",
    "    print('=======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [[word for word in document.lower().split() if word not in stop] for document in df_news.text[:100]]\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda = LdaModel(corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  '0.019*\"собора\" + 0.013*\"рпц\" + 0.011*\"-\" + 0.010*\"исаакиевского\" + 0.009*\"передачи\" + 0.008*\"против\" + 0.007*\"собор\" + 0.006*\"января\" + 0.006*\"13\" + 0.005*\"передаче\"'),\n",
       " (3,\n",
       "  '0.012*\"собора\" + 0.011*\"-\" + 0.008*\"передачи\" + 0.007*\"рпц\" + 0.005*\"исаакиевского\" + 0.005*\"это\" + 0.004*\"церкви\" + 0.004*\"передаче\" + 0.004*\"января\" + 0.004*\"собор\"'),\n",
       " (6,\n",
       "  '0.022*\"собора\" + 0.018*\"исаакиевского\" + 0.016*\"передачи\" + 0.011*\"рпц\" + 0.011*\"против\" + 0.011*\"января\" + 0.010*\"13\" + 0.010*\"-\" + 0.008*\"2017\" + 0.008*\"собор\"'),\n",
       " (0,\n",
       "  '0.009*\"-\" + 0.009*\"собора\" + 0.008*\"передачи\" + 0.007*\"против\" + 0.006*\"рпц\" + 0.006*\"собор\" + 0.006*\"исаакиевского\" + 0.005*\"января\" + 0.005*\"13\" + 0.005*\"это\"'),\n",
       " (7,\n",
       "  '0.014*\"исаакиевского\" + 0.013*\"рпц\" + 0.012*\"собора\" + 0.011*\"-\" + 0.010*\"передачи\" + 0.009*\"против\" + 0.008*\"января\" + 0.006*\"13\" + 0.006*\"акция\" + 0.005*\"исаакиевский\"')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
