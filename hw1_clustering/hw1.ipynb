{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from re import sub\n",
    "from pandas import DataFrame\n",
    "from pymystem3 import Mystem\n",
    "import texterra\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from itertools import chain\n",
    "from pickle import dump, load\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import SpectralClustering, AgglomerativeClustering, KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import *\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "API_KEY = '9988cfb979b80264baeba1386cc7e455f99f943c'\n",
    "\n",
    "stop = stopwords.words('russian')\n",
    "la = np.linalg\n",
    "morph = MorphAnalyzer()\n",
    "m = Mystem()\n",
    "# t = texterra.API(API_KEY)\n",
    "alpha_tokenizer = RegexpTokenizer('[A-Za-zА-Яа-я]\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame, concat\n",
    "from os import path\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "# import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 1 [10 баллов] \n",
    "# До 1.12.17 23:59\n",
    "\n",
    "Задание выполняется в группе (1-4 человека). В случае использования какого-либо строннего источника информации обязательно дайте на него ссылку (поскольку другие тоже могут на него наткнуться). Плагиат наказывается нулём баллов за задание и предвзятым отношением в будущем.\n",
    "\n",
    "Не все части обязательны для выполнения, однако вы можете быть дополнительно оштрафованы за небрежное за выполнение одной или двух частей вместо четырех.\n",
    "\n",
    "При возниконовении проблем с выполнением задания обращайтесь с вопросами к преподавателю. Поэтому настоятельно рекомендуется выполнять задание заранее, оставив запас времени на всевозможные технические проблемы. Если вы начали читать условие в последний вечер и не успели из-за проблем с установкой какой-либо библиотеки — это ваши проблемы.\n",
    "\n",
    "\n",
    "Результат выполнения задания — это отчёт в формате html на основе Jupyter Notebook. Нормальный отчёт должен включать в себя:\n",
    "* Краткую постановку задачи и формулировку задания\n",
    "* Описание **минимума** необходимой теории и/или описание используемых инструментов - не стоит переписывать лекции или Википедию\n",
    "* Подробный пошаговый рассказ о проделанной работе\n",
    "* Аккуратно оформленные результаты\n",
    "* **Внятные выводы** – не стоит относится к домашнему заданию как к последовательности сугубо технических шагов, а стоит относится скорее как к небольшому практическому исследованию, у которого есть своя цель и свое назначение.\n",
    "\n",
    "Небрежное его оформление отчета существенно отразится на итоговой оценке. Весь код из отчёта должен быть воспроизводимым, если для этого нужны какие-то дополнительные действия, установленные модули и т.п. — всё это должно быть прописано в тексте в явном виде.\n",
    "\n",
    "Сдача отчетов осуществляется через систему AnyTask.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация новостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входная коллекция данных состоит из двух частей:\n",
    "1. events.csv – список 28 резонансных событий первой половины 2017 года, каждому событию присвоен свой порядковый номер (id)\n",
    "2. raw_news.csv – тексты новостей из различных новостных источников, известно, к какому резонансному событию относится каждая новость (столбец event_id).\n",
    "\n",
    "Будем считать, что одно событие – это один кластер. В этом домашнем задании вам предстоит:\n",
    "1. провести кластеризацию текстов новостей и проверить, получается ли восстановить кластерную структуру \n",
    "2. проверить, можно ли использовать кластерный анализ для обобщения: найти небольшое число кластеров и проверить, получается ли выделить общие направлени новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_events = DataFrame.from_csv('events.csv')\n",
    "df_news = DataFrame.from_csv('raw_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>В ПЕТЕРБУРГЕ ПРОШЕЛ МИТИНГ ПРОТИВ ПЕРЕДАЧИ ИС...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lenta.co, Москва, 14 января 2017 СИТУАЦИЯ С П...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Аргументы и Факты (aif.ru), Москва, 14 января...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Google Новости ТОП, Москва, 14 января 2017 АК...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Газета.Ru, Москва, 13 января 2017 В МОСКОВСКО...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id                                               text\n",
       "0         1   В ПЕТЕРБУРГЕ ПРОШЕЛ МИТИНГ ПРОТИВ ПЕРЕДАЧИ ИС...\n",
       "1         1   Lenta.co, Москва, 14 января 2017 СИТУАЦИЯ С П...\n",
       "2         1   Аргументы и Факты (aif.ru), Москва, 14 января...\n",
       "3         1   Google Новости ТОП, Москва, 14 января 2017 АК...\n",
       "4         1   Газета.Ru, Москва, 13 января 2017 В МОСКОВСКО..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1 [2 балла] Предварительная обработка текстов\n",
    "Проведите предобработку новостей: токенизацию, приведение к нижнему регистру, лемматизацию. Проверьте, есть ли в коллекции дубликаты. Посчитайте, сколько новостей относится к каждому резонансному событию. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Для начала смерджим все тексты новостей для того, чтобы получить один единый корпус, с которым можно работать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ' '.join(df_news.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистка новостей от шумов (пока реализовано только удаление УРЛОВ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return sub(r'http\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = remove_url(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация (удаляем всё, что не является alphanumeric-символами):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = alpha_tokenizer.tokenize(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация (и автоматические приведение к нижнему регистру). Мы решили рассмотреть три варианта лемматизации, поскольку не все способны одинаково хорошо справляться с контекстной омонимией (как, например, во фразе \"Запотело стекло. Варенье стекло по краю банки.\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# norm_tokens = [morph.parse(word)[0].normal_form for word in tokens]\n",
    "\n",
    "with open('lemmas_from_texterra.pickle', 'rb') as f:\n",
    "    texts = load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сколько новостей относится к каждому резонансному событию?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17    102\n",
       "28    100\n",
       "18    100\n",
       "7     100\n",
       "10    100\n",
       "12    100\n",
       "27    100\n",
       "16    100\n",
       "1     100\n",
       "25    100\n",
       "26    100\n",
       "21    100\n",
       "23    100\n",
       "3      84\n",
       "22     82\n",
       "9      82\n",
       "24     62\n",
       "4      62\n",
       "2      51\n",
       "11     49\n",
       "19     45\n",
       "6      41\n",
       "8      27\n",
       "13     24\n",
       "20      8\n",
       "15      7\n",
       "5       2\n",
       "14      2\n",
       "Name: event_id, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.event_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ли в коллекции дубликаты?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     event_id                                               text\n",
       "513         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "518         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "522         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "526         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "528         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "536         9   Коммерсантъ. Новости информ. центра, Москва, ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news[df_news.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2 [3 балла] Кластеризация текстов\n",
    "Любым известным вам алгоритмом найдите в коллекции новостей 28 кластеров. Используйте $tf-idf$ преобразование, сингулярное разложение, нормировку признакого пространства и любые другие техники обработки данных, которые вы считаете нужным. Используйте любые известные вам меры качества для оценки качества кластеризации. Покажите, как  $tf-idf$ преобразование или сингулярное разложение или любая другая использованная вами техника влияет на качество кластеризации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем Truncated SVD (a.k.a Latent Semantic Analysis):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = [' '.join([word for word in sentence]) for sentence in texts]\n",
    "ngram_range=(1,1)\n",
    "n_clusters=28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectroizer: TF-IDF Vectorizer, Clusterizer: AgglomerativeClustering \n",
      "ARI = 0.46, V = 0.75, completenss = 0.74, homogenity = 0.74 \n",
      "======\n"
     ]
    }
   ],
   "source": [
    "for vectorizer, vectorizer_name in [\n",
    "                        (TfidfVectorizer(ngram_range=ngram_range), 'TF-IDF Vectorizer'),\n",
    "                          #(CountVectorizer(ngram_range=ngram_range), 'Count Vectorizer'), \n",
    "                          #(HashingVectorizer(ngram_range=ngram_range), 'Hashing Vectorizer')\n",
    "                        ]:\n",
    "    count_model = vectorizer\n",
    "    X = count_model.fit_transform(words_list)\n",
    "    svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "    U = svd.fit_transform(X)  \n",
    "    for clusterizer, clusterizer_name in [\n",
    "                                        #(SpectralClustering(n_clusters=n_clusters), 'Spectral Clustering'),\n",
    "                                         #(KMeans(n_clusters=n_clusters), 'KMeans'),\n",
    "                                         (AgglomerativeClustering(n_clusters=n_clusters), 'AgglomerativeClustering')\n",
    "                                        ]:\n",
    "        clusterizer_model = clusterizer.fit(U)\n",
    "        print('Vectroizer: {}, Clusterizer: {} \\nARI = {:0.2f}, V = {:0.2f}, completenss = {:0.2f}, homogenity = {:0.2f} \\n======'.format(\n",
    "                    vectorizer_name,\n",
    "                    clusterizer_name,\n",
    "                    adjusted_rand_score(clusterizer_model.labels_, df_news.event_id),\n",
    "                    completeness_score(clusterizer_model.labels_, df_news.event_id),\n",
    "                    v_measure_score(clusterizer_model.labels_, df_news.event_id),\n",
    "                    homogeneity_score(clusterizer_model.labels_, df_news.event_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем Normalizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectroizer: TF-IDF Vectorizer, Clusterizer: Spectral Clustering \n",
      "ARI = 0.62, V = 0.84, completenss = 0.85, homogenity = 0.87 \n",
      "======\n",
      "Vectroizer: TF-IDF Vectorizer, Clusterizer: KMeans \n",
      "ARI = 0.87, V = 0.95, completenss = 0.94, homogenity = 0.92 \n",
      "======\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a3ab5158d566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                          \u001b[0;34m(\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KMeans'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                          (AgglomerativeClustering(n_clusters=n_clusters), 'AgglomerativeClustering')]:\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mspectral\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusterizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         print('Vectroizer: {}, Clusterizer: {} \\nARI = {:0.2f}, V = {:0.2f}, completenss = {:0.2f}, homogenity = {:0.2f} \\n======'.format(\n\u001b[1;32m     13\u001b[0m                     \u001b[0mvectorizer_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/cluster/hierarchical.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    740\u001b[0m             memory.cache(tree_builder)(X, connectivity,\n\u001b[1;32m    741\u001b[0m                                        \u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                                        **kwargs)\n\u001b[0m\u001b[1;32m    743\u001b[0m         \u001b[0;31m# Cut the tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompute_full_tree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/cluster/hierarchical.py\u001b[0m in \u001b[0;36mward_tree\u001b[0;34m(X, connectivity, n_clusters, return_distance)\u001b[0m\n\u001b[1;32m    181\u001b[0m                           \u001b[0;34m'for the specified number of clusters'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                           stacklevel=2)\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mchildren_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/cluster/hierarchy.py\u001b[0m in \u001b[0;36mward\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \"\"\"\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/cluster/hierarchy.py\u001b[0m in \u001b[0;36mlinkage\u001b[0;34m(y, method, metric, optimal_ordering)\u001b[0m\n\u001b[1;32m    706\u001b[0m                          \u001b[0;34m'matrix looks suspiciously like an uncondensed '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                          'distance matrix')\n\u001b[0;32m--> 708\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`y` must be 1 or 2 dimensional.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mpdist\u001b[0;34m(X, metric, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m             pdist_fn = getattr(_distance_wrap,\n\u001b[1;32m   1713\u001b[0m                                \"pdist_%s_%s_wrap\" % (metric_name, typ))\n\u001b[0;32m-> 1714\u001b[0;31m             \u001b[0mpdist_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for vectorizer, vectorizer_name in [(TfidfVectorizer(ngram_range=ngram_range), 'TF-IDF Vectorizer'),\n",
    "                          (CountVectorizer(ngram_range=ngram_range), 'Count Vectorizer'), \n",
    "                          (HashingVectorizer(ngram_range=ngram_range), 'Hashing Vectorizer')]:\n",
    "    count_model = vectorizer\n",
    "    X = count_model.fit_transform(words_list)\n",
    "    normalizer = Normalizer()\n",
    "    U = normalizer.fit_transform(X)\n",
    "    for clusterizer, clusterizer_name in [(SpectralClustering(n_clusters=n_clusters), 'Spectral Clustering'),\n",
    "                                         (KMeans(n_clusters=n_clusters), 'KMeans'),\n",
    "                                         (AgglomerativeClustering(n_clusters=n_clusters), 'AgglomerativeClustering')]:\n",
    "        spectral = clusterizer.fit(U.toarray())\n",
    "        print('Vectroizer: {}, Clusterizer: {} \\nARI = {:0.2f}, V = {:0.2f}, completenss = {:0.2f}, homogenity = {:0.2f} \\n======'.format(\n",
    "                    vectorizer_name,\n",
    "                    clusterizer_name,\n",
    "                    adjusted_rand_score(spectral.labels_, df_news.event_id),\n",
    "                    completeness_score(spectral.labels_, df_news.event_id),\n",
    "                    v_measure_score(spectral.labels_, df_news.event_id),\n",
    "                    homogeneity_score(spectral.labels_, df_news.event_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3 [5 баллов] Кластеризация текстов (продолжение)\n",
    "Задайте число кластеров заведомо меньше 28 (например, 5 кластеров). Повторно кластеризуйте новости. Можете ли вы проинтепретировать полученные кластеры? Получается ли с помощью кластерного анализа определить в каких сферах жизни (например, культура, политика, спорт) произошли события?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "X = vectorizer.fit_transform(words_list)\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "U = svd.fit_transform(X) \n",
    "clusterizer = SpectralClustering(n_clusters=5)\n",
    "clusterizer_model = clusterizer.fit(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clusterizer_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_merged = []\n",
    "\n",
    "for event in range(5):\n",
    "    events = set([df_news.event_id.values[i] for i, x in enumerate(labels) if x == event])\n",
    "    events_merged.append(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дональд Трамп вступил в должность президента США.\n",
      "CNN показала фильм «Владимир Путин — самый влиятельный человек в мире».\n",
      "Юлию Самойлову не пустили на евровидении в Киеве\n",
      "В центре Киева был убит бывший депутат Госдумы РФ от КПРФ Денис Вороненков\n",
      "Тиллерсон посещает Москву и встречается с Путиным\n",
      "Несанкционированные акции в Москве апрель\n",
      "Путин и Меркель в Сочи\n",
      "Победа Макрона во Франции\n",
      "Горячая линия Президента Путина\n",
      "Саммит G20\n",
      "Единый день голосования\n",
      "=======\n",
      "Дональд Трамп вступил в должность президента США.\n",
      "Скоропостижно скончался постпред России при ООН Виталий Чуркин.\n",
      "Вышел фильм Навального «он Вам не димон»\n",
      "Умер Дэвид рокфеллер\n",
      "теракт произошел в центре Лондона\n",
      "Юлию Самойлову не пустили на евровидении в Киеве\n",
      "В центре Киева был убит бывший депутат Госдумы РФ от КПРФ Денис Вороненков\n",
      "Митинг в москве против коррупции\n",
      "SpaceX впервые в истории запустила и посадила уже летавшую ракету-носитель\n",
      "Умер Евгений Евтушенко\n",
      "Премьер Медведев выступает перед депутатами Госдумы с отчетом об итогах работы правительства за 2016 год\n",
      "Несанкционированные акции в Москве апрель\n",
      "Чемпионат мира по хоккею\n",
      "Победа Макрона во Франции\n",
      "Митинг против Реновации в Москве\n",
      "Ураган в Москве\n",
      "Парламентские выборы в Великобритании\n",
      "Горячая линия Президента Путина\n",
      "Кубок конфедерации FiFA\n",
      "Теракт в Барселоне\n",
      "Единый день голосования\n",
      "=======\n",
      "Власти Петербурга согласились передать РПЦ Исаакиевский собор.\n",
      "Вышел фильм Навального «он Вам не димон»\n",
      "Митинг в москве против коррупции\n",
      "Несанкционированные акции в Москве апрель\n",
      "Митинг против Реновации в Москве\n",
      "Акции протеста 12 июня\n",
      "Кубок конфедерации FiFA\n",
      "=======\n",
      "Скоропостижно скончался постпред России при ООН Виталий Чуркин.\n",
      "Умер Евгений Евтушенко\n",
      "Умер Дэвид рокфеллер\n",
      "=======\n",
      "Правительство внесло в Госдуму законопроект о курортных сборах\n",
      "=======\n"
     ]
    }
   ],
   "source": [
    "for events_set in events_merged:\n",
    "    for event in events_set:\n",
    "        print(df_events.loc[df_events.index == event]['name'].values[0])\n",
    "    print('=======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in document.lower().split() if word not in stop] for document in df_news.text[:100]]\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda = LdaModel(corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4,\n",
       "  '0.016*\"-\" + 0.013*\"собора\" + 0.011*\"исаакиевского\" + 0.007*\"против\" + 0.006*\"13\" + 0.006*\"рпц\" + 0.006*\"января\" + 0.006*\"передачи\" + 0.005*\"это\" + 0.005*\"собор\"'),\n",
       " (1,\n",
       "  '0.020*\"собора\" + 0.015*\"передачи\" + 0.014*\"рпц\" + 0.013*\"исаакиевского\" + 0.012*\"-\" + 0.010*\"собор\" + 0.007*\"января\" + 0.007*\"исаакиевский\" + 0.007*\"13\" + 0.007*\"против\"'),\n",
       " (9,\n",
       "  '0.011*\"-\" + 0.010*\"собора\" + 0.007*\"исаакиевского\" + 0.005*\"собор\" + 0.005*\"января\" + 0.005*\"рпц\" + 0.005*\"исаакиевский\" + 0.005*\"это\" + 0.005*\"передачи\" + 0.004*\"13\"'),\n",
       " (8,\n",
       "  '0.012*\"исаакиевского\" + 0.011*\"собора\" + 0.010*\"января\" + 0.010*\"передачи\" + 0.009*\"-\" + 0.009*\"рпц\" + 0.008*\"2017\" + 0.007*\"13\" + 0.007*\"против\" + 0.006*\"безвозмездное\"'),\n",
       " (7,\n",
       "  '0.014*\"собора\" + 0.013*\"-\" + 0.010*\"исаакиевского\" + 0.009*\"передачи\" + 0.008*\"рпц\" + 0.008*\"января\" + 0.007*\"собор\" + 0.007*\"13\" + 0.005*\"против\" + 0.005*\"исаакиевский\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
