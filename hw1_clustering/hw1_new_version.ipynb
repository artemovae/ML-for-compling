{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Машинное обучение. Кластеризация новостных текстов</center></h1>\n",
    "<center>Никиша Ирина, Степачёв Павел, Бакаров Амир</center>\n",
    "<center>Национальный исследовательский университет \"Высшая школа экономики\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Данное исследование посвящено сравению различных алгоритмов векторизации и кластеризации в задаче кластеризации набора новостных текстов. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Введение\n",
    "\n",
    "Кластерный анализ (англ. cluster analysis) — многомерная статистическая процедура, выполняющая сбор данных, содержащих информацию о выборке объектов, и затем упорядочивающая объекты в сравнительно однородные группы. Задача кластеризации относится к статистической обработке, а также к широкому классу задач обучения без учителя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проведения экспериментов с кластеризацией новостного корпуса потребуются следующие библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from os import path\n",
    "import glob\n",
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from re import sub\n",
    "from pandas import DataFrame\n",
    "from pymystem3 import Mystem\n",
    "import texterra\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import codecs\n",
    "import csv\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import *\n",
    "import scipy.cluster.hierarchy as hac\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API_KEY = '9988cfb979b80264baeba1386cc7e455f99f943c'\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "m = Mystem()\n",
    "t = texterra.API(API_KEY)\n",
    "alpha_tokenizer = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ данных\n",
    "\n",
    "Новостной корпус представлен данными в файлах 'events.csv' и 'raw_news.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_events = DataFrame.from_csv('events.csv')\n",
    "df_news = DataFrame.from_csv('raw_news.csv')\n",
    "texts = list(df_news.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные имеют следующий вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>В ПЕТЕРБУРГЕ ПРОШЕЛ МИТИНГ ПРОТИВ ПЕРЕДАЧИ ИС...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lenta.co, Москва, 14 января 2017 СИТУАЦИЯ С П...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Аргументы и Факты (aif.ru), Москва, 14 января...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Google Новости ТОП, Москва, 14 января 2017 АК...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Газета.Ru, Москва, 13 января 2017 В МОСКОВСКО...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id                                               text\n",
       "0         1   В ПЕТЕРБУРГЕ ПРОШЕЛ МИТИНГ ПРОТИВ ПЕРЕДАЧИ ИС...\n",
       "1         1   Lenta.co, Москва, 14 января 2017 СИТУАЦИЯ С П...\n",
       "2         1   Аргументы и Факты (aif.ru), Москва, 14 января...\n",
       "3         1   Google Новости ТОП, Москва, 14 января 2017 АК...\n",
       "4         1   Газета.Ru, Москва, 13 января 2017 В МОСКОВСКО..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17    102\n",
       "28    100\n",
       "18    100\n",
       "7     100\n",
       "10    100\n",
       "12    100\n",
       "27    100\n",
       "16    100\n",
       "1     100\n",
       "25    100\n",
       "26    100\n",
       "21    100\n",
       "23    100\n",
       "3      84\n",
       "22     82\n",
       "9      82\n",
       "24     62\n",
       "4      62\n",
       "2      51\n",
       "11     49\n",
       "19     45\n",
       "6      41\n",
       "8      27\n",
       "13     24\n",
       "20      8\n",
       "15      7\n",
       "5       2\n",
       "14      2\n",
       "Name: event_id, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.event_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также выяснили, есть ли в корпусе дубликаты, и избавились от них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     event_id                                               text\n",
       "513         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "518         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "522         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "526         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "528         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "536         9   Коммерсантъ. Новости информ. центра, Москва, ..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news[df_news.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = df_news.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка текста\n",
    "\n",
    "Лемматизация (и автоматические приведение к нижнему регистру). Мы решили рассмотреть три варианта лемматизации, поскольку не все способны одинаково хорошо справляться с контекстной омонимией (как, например, во фразе \"Поля в поле моет пол и заполнила пол поля\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_with_pymorphy(tokens):\n",
    "    return [morph.parse(word)[0].normal_form for word in tokens]\n",
    "\n",
    "def normalize_with_mystem(tokens):\n",
    "    return ''.join(m.lemmatize(' '.join(tokens))).split()\n",
    "\n",
    "def normalize_with_texterra(tokens):\n",
    "    time.sleep(10)\n",
    "    text = ' '.join(tokens)\n",
    "    return [token[3] for token in list(t.lemmatization(text))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pymorphy2: 0.90 (поль в пол мыть половина и заполнить половина поль)\n",
      "MyStem: 0.87 (поле в поле мыть пол и заполнять пол поля)\n",
      "Texterra: 0.88 (поле в поле мывать пол и заполнять пол поле)\n"
     ]
    }
   ],
   "source": [
    "tricky_text = 'Поля в поле моет пол и заполнила пол поля'\n",
    "gold_standard_normalized = 'поля в поле мыть пол и заполнить половина поле'\n",
    "\n",
    "for normalizer, normalize in [('Pymorphy2', normalize_with_pymorphy),\n",
    "                              ('MyStem', normalize_with_mystem),\n",
    "                              ('Texterra', normalize_with_texterra)]:\n",
    "    normalized_text = ' '.join(normalize((alpha_tokenizer.tokenize(tricky_text))))\n",
    "    print('{}: {:0.2f} ({})'.format(normalizer, SequenceMatcher(None, gold_standard_normalized, normalized_text).ratio(), normalized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы представим лемматизированные данные в виде трёх наборов данных и проверим, на каком из них рассмотренные алгоритмы покажут наилучшие результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_type_of_tokens = {}\n",
    "tokenizers = {'pymorphy': normalize_with_pymorphy, 'texterra': normalize_with_texterra, 'mystem': normalize_with_mystem}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные предварительно сериализованы в .pickle-файлы для того, чтобы впоследствии к ним можно было удобно обращаться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, tokenizer in tokenizers.items():\n",
    "    all_texts = []\n",
    "    for text in texts:\n",
    "        # print(texts.index(text)) Может, не надо их принтовать?\n",
    "        text = sub(r'http\\S+', '', text)\n",
    "        tokens = alpha_tokenizer.tokenize(text)\n",
    "        tokens = tokenizer(tokens)\n",
    "        all_texts.append(tokens)\n",
    "    # print(all_texts) Может, не надо их принтовать?\n",
    "    with open('tokens_from_' + name + '.pickle', 'wb') as f:\n",
    "        pickle.dump(all_texts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем наши леммы в словарь типа ```data = {'texterra' : [ [lemma1, lemma2 ...], [], [] ... [] ],  'mystem': [ ], etc.}```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "for filename in glob.iglob(path.join(path.dirname(__file__),'lemmas_from_*.pickle'), recursive=True):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data_lemmas = pickle.load(f)\n",
    "        data[path.splitext(path.basename(filename))[0]] = data_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация\n",
    "\n",
    "Далее для каждого типа лемм создаем W2V модели, матрицу векторов для всех текстов, сохраняем ее в pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_features = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name_of_alg, list_of_docs in data.items():\n",
    "    \n",
    "    model = gensim.models.Word2Vec(list_of_docs, size=num_of_features, min_count=30, window=30)\n",
    "    #model.save(name_of_alg.replace('lemmas','w2v_model2')+'.mdl')\n",
    "    \n",
    "    vectors_list = []\n",
    "    \n",
    "    for text_id in range(len(list_of_docs)):\n",
    "        vector_for_each_text = []\n",
    "        \n",
    "        for word in list_of_docs[text_id]:\n",
    "            try:\n",
    "                featureVec = np.zeros(shape=(1, num_of_features), dtype='float32')\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "                vector_for_each_text.append(featureVec)\n",
    "                \n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        first_vector = np.array(vector_for_each_text[0])\n",
    "        for i in range(1, len(vector_for_each_text)):\n",
    "            first_vector = np.add(first_vector, vector_for_each_text[i])\n",
    "            \n",
    "        resultVec = np.divide(first_vector, len(vector_for_each_text))\n",
    "        vectors_list.append(resultVec)\n",
    "\n",
    "    vectors_array = np.array(vectors_list[0])\n",
    "    for i in range(1, len(vectors_list)):\n",
    "        vectors_array = np.vstack((vectors_array,vectors_list[i]))\n",
    "\n",
    "    with open(name_of_alg.replace('lemmas', 'vectors2')+ '.pickle', 'wb') as f:\n",
    "            pickle.dump(np.matrix(vectors_array), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое происходит с Doc2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name_of_alg, list_of_docs in data.items():\n",
    "    \n",
    "    #тут короче преобразования для того, чтобы doc2vec нормально кушал тексты \n",
    "    sentences = [gensim.models.doc2vec.TaggedDocument(words=list_of_docs[doc], tags=[u'text']) for doc \n",
    "                 in range(len(list_of_docs))]\n",
    "    model = gensim.models.doc2vec.Doc2Vec(sentences, size=num_of_features, min_count=70, window=70)\n",
    "    #model.save(name_of_alg.replace('lemmas', 'd2v_model') + '.mdl')\n",
    "    model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "\n",
    "    vectors_list = []\n",
    "    for text_id in range(len(list_of_docs)):\n",
    "        vector_for_each_text = []\n",
    "        for word in list_of_docs[text_id]:\n",
    "            try:\n",
    "                featureVec = np.zeros(shape=(1, num_of_features), dtype='float32')\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "                vector_for_each_text.append(featureVec)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        first = np.array(vector_for_each_text[0])\n",
    "\n",
    "        for i in range(1, len(vector_for_each_text)):\n",
    "            first = np.add(first, vector_for_each_text[i])\n",
    "        resultVec = np.divide(first, len(vector_for_each_text))\n",
    "        vectors_list.append(resultVec)\n",
    "\n",
    "    vectors_array = np.array(vectors_list[0])\n",
    "\n",
    "    for i in range(1, len(vectors_list)):\n",
    "        vectors_array = np.vstack((vectors_array, vectors_list[i]))\n",
    "        \n",
    "    with open(name_of_alg.replace('lemmas', 'vectors_d2v') + '.pickle', 'wb') as f:\n",
    "            pickle.dump(np.matrix(vectors_array), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация\n",
    "\n",
    "Мы используем три алгоритым кластеризации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_alg = [KMeans, AgglomerativeClustering, SpectralClustering]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для отрисовки Agglomerative clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotClusters(a):\n",
    "    z = hac.linkage(a, method='ward')\n",
    "    hac.dendrogram(z)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки используем \"настоящие\" метки новостей из ```raw_news.csv```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = list(df_news.event_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь для каждого типа лемм применяем PipeLine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in glob.iglob('vectors_d2v_from_*.pickle', recursive=True):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data_model = pickle.load(f)\n",
    "        \n",
    "        for alg in list_of_alg:\n",
    "            print(alg, filename.replace('vectors_d2v_from_', '').replace('.pickle',''))\n",
    "            \n",
    "            pipeline = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                                 ('svd', TruncatedSVD(n_components=150)),\n",
    "                                 ('norm', Normalizer()),\n",
    "                                 ('clust', alg(n_clusters=28))\n",
    "                                ])\n",
    "            pipeline.fit(data_model)\n",
    "            \n",
    "            explained_variance = pipeline.named_steps['svd'].explained_variance_ratio_.sum()\n",
    "            print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "                        \n",
    "            clust_labels = pipeline.named_steps['clust'].labels_\n",
    "\n",
    "            print(\"Homogeneity:\", homogeneity_score(labels, clust_labels))\n",
    "            print(\"Completeness:\", completeness_score(labels, clust_labels))\n",
    "            print(\"V-measure\",  v_measure_score(labels, clust_labels))\n",
    "            print(\"Adjusted Rand-Index:\",  adjusted_rand_score(labels, clust_labels))\n",
    "            print(confusion_matrix(labels, clust_labels))\n",
    "            print('==============')\n",
    "        \n",
    "        if alg == AgglomerativeClustering:\n",
    "            plotClusters(data_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот тут вот сверху confusion matrix, если мы будем рисовать ее..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обсуждение\n",
    "\n",
    "Полученные результаты говорят нам о том, что в большинстве случаев полученные с помощью алгоритмов метки кластеров были похожи на метки кластеров, проставленные людьми; при этом пока ничего не понятно о том, действительно ли полученные кластеры будут интерпретируемы. Для этого мы решеили попробовать разбить данные на 5 кластеров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_from_pymorphy.pickle', 'rb') as f:\n",
    "    tokens = pickle.load(f)\n",
    "words_list = [' '.join([word for word in sentence]) for sentence in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(words_list)\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "U = svd.fit_transform(X) \n",
    "clusterizer = SpectralClustering(n_clusters=5)\n",
    "clusterizer_model = clusterizer.fit(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = clusterizer_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events_merged = []\n",
    "\n",
    "for event in range(5):\n",
    "    events = set([df_news.event_id.values[i] for i, x in enumerate(labels) if x == event])\n",
    "    events_merged.append(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дональд Трамп вступил в должность президента США.\n",
      "Вышел фильм Навального «он Вам не димон»\n",
      "Умер Дэвид рокфеллер\n",
      "теракт произошел в центре Лондона\n",
      "Юлию Самойлову не пустили на евровидении в Киеве\n",
      "В центре Киева был убит бывший депутат Госдумы РФ от КПРФ Денис Вороненков\n",
      "Митинг в москве против коррупции\n",
      "SpaceX впервые в истории запустила и посадила уже летавшую ракету-носитель\n",
      "Умер Евгений Евтушенко\n",
      "Премьер Медведев выступает перед депутатами Госдумы с отчетом об итогах работы правительства за 2016 год\n",
      "Несанкционированные акции в Москве апрель\n",
      "Чемпионат мира по хоккею\n",
      "Победа Макрона во Франции\n",
      "Митинг против Реновации в Москве\n",
      "Ураган в Москве\n",
      "Парламентские выборы в Великобритании\n",
      "Горячая линия Президента Путина\n",
      "Кубок конфедерации FiFA\n",
      "Теракт в Барселоне\n",
      "Единый день голосования\n",
      "=======\n",
      "Дональд Трамп вступил в должность президента США.\n",
      "CNN показала фильм «Владимир Путин — самый влиятельный человек в мире».\n",
      "Юлию Самойлову не пустили на евровидении в Киеве\n",
      "В центре Киева был убит бывший депутат Госдумы РФ от КПРФ Денис Вороненков\n",
      "Тиллерсон посещает Москву и встречается с Путиным\n",
      "Несанкционированные акции в Москве апрель\n",
      "Путин и Меркель в Сочи\n",
      "Победа Макрона во Франции\n",
      "Парламентские выборы в Великобритании\n",
      "Горячая линия Президента Путина\n",
      "Саммит G20\n",
      "=======\n",
      "Правительство внесло в Госдуму законопроект о курортных сборах\n",
      "=======\n",
      "Скоропостижно скончался постпред России при ООН Виталий Чуркин.\n",
      "Умер Евгений Евтушенко\n",
      "Умер Дэвид рокфеллер\n",
      "=======\n",
      "Власти Петербурга согласились передать РПЦ Исаакиевский собор.\n",
      "Вышел фильм Навального «он Вам не димон»\n",
      "Митинг в москве против коррупции\n",
      "Несанкционированные акции в Москве апрель\n",
      "Митинг против Реновации в Москве\n",
      "Акции протеста 12 июня\n",
      "Кубок конфедерации FiFA\n",
      "=======\n"
     ]
    }
   ],
   "source": [
    "for events_set in events_merged:\n",
    "    for event in events_set:\n",
    "        print(df_events.loc[df_events.index == event]['name'].values[0])\n",
    "    print('=======')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Вот так вот"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение\n",
    "\n",
    "Таким образом,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
