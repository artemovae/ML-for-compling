{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Машинное обучение. Кластеризация новостных текстов</center></h1>\n",
    "<center>Никиша Ирина, Степачёв Павел, Бакаров Амир</center>\n",
    "<center>Национальный исследовательский университет \"Высшая школа экономики\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Данное исследование посвящено сравению различных алгоритмов векторизации и кластеризации в задаче кластеризации набора новостных текстов. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Введение\n",
    "\n",
    "Кластерный анализ (англ. cluster analysis) — многомерная статистическая процедура, выполняющая сбор данных, содержащих информацию о выборке объектов, и затем упорядочивающая объекты в сравнительно однородные группы. Задача кластеризации относится к статистической обработке, а также к широкому классу задач обучения без учителя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проведения экспериментов с кластеризацией новостного корпуса потребуются следующие библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from os import path\n",
    "import glob\n",
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from re import sub\n",
    "from pandas import DataFrame, options\n",
    "from pymystem3 import Mystem\n",
    "import texterra\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import codecs\n",
    "import csv\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import *\n",
    "import scipy.cluster.hierarchy as hac\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API_KEY = '9988cfb979b80264baeba1386cc7e455f99f943c'\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "m = Mystem()\n",
    "t = texterra.API(API_KEY)\n",
    "alpha_tokenizer = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ данных\n",
    "\n",
    "Новостной корпус представлен данными в файлах 'events.csv' и 'raw_news.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_events = DataFrame.from_csv('events.csv')\n",
    "df_news = DataFrame.from_csv('raw_news.csv')\n",
    "texts = list(df_news.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные имеют следующий вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>В ПЕТЕРБУРГЕ ПРОШЕЛ МИТИНГ ПРОТИВ ПЕРЕДАЧИ ИСААКИЕВСКОГО СОБОРА РПЦ В Санкт-Петербурге люди устроили акцию протеста...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lenta.co, Москва, 14 января 2017 СИТУАЦИЯ С ПЕРЕДАЧЕЙ ИСААКИЕВСКОГО СОБОРА В ВЕДЕНИЕ РПЦ. ИСТОРИЯ СОБОРА Читать ори...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Аргументы и Факты (aif.ru), Москва, 14 января 2017 ОППОЗИЦИЯ ПРОВЕЛА МИТИНГ ПРОТИВ ПЕРЕДАЧИ ИСААКИЕВСКОГО СОБОРА РП...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Google Новости ТОП, Москва, 14 января 2017 АКЦИЯ ПРОТИВ ПЕРЕДАЧИ ИСААКИЕВСКОГО СОБОРА РПЦ ПРОШЛА БЕЗ НАРУШЕНИЙ Моск...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Газета.Ru, Москва, 13 января 2017 В МОСКОВСКОЙ ПАТРИАРХИИ ПРОКОММЕНТИРОВАЛИ ПЕРЕДАЧУ ИСААКИЕВСКОГО СОБОРА РПЦ Руков...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id  \\\n",
       "0         1   \n",
       "1         1   \n",
       "2         1   \n",
       "3         1   \n",
       "4         1   \n",
       "\n",
       "                                                                                                                      text  \n",
       "0   В ПЕТЕРБУРГЕ ПРОШЕЛ МИТИНГ ПРОТИВ ПЕРЕДАЧИ ИСААКИЕВСКОГО СОБОРА РПЦ В Санкт-Петербурге люди устроили акцию протеста...  \n",
       "1   Lenta.co, Москва, 14 января 2017 СИТУАЦИЯ С ПЕРЕДАЧЕЙ ИСААКИЕВСКОГО СОБОРА В ВЕДЕНИЕ РПЦ. ИСТОРИЯ СОБОРА Читать ори...  \n",
       "2   Аргументы и Факты (aif.ru), Москва, 14 января 2017 ОППОЗИЦИЯ ПРОВЕЛА МИТИНГ ПРОТИВ ПЕРЕДАЧИ ИСААКИЕВСКОГО СОБОРА РП...  \n",
       "3   Google Новости ТОП, Москва, 14 января 2017 АКЦИЯ ПРОТИВ ПЕРЕДАЧИ ИСААКИЕВСКОГО СОБОРА РПЦ ПРОШЛА БЕЗ НАРУШЕНИЙ Моск...  \n",
       "4   Газета.Ru, Москва, 13 января 2017 В МОСКОВСКОЙ ПАТРИАРХИИ ПРОКОММЕНТИРОВАЛИ ПЕРЕДАЧУ ИСААКИЕВСКОГО СОБОРА РПЦ Руков...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options.display.max_colwidth = 120\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, каждая новость представлена в виде фрагмента текста и имеет метку о принадлежности к определенному событию. Одному событию может соотвествовать несколько новостей, но каждая новости ассоциировано только одно событие; диаграмма ниже показывает, как распределены события по новостям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17    102\n",
       "28    100\n",
       "18    100\n",
       "7     100\n",
       "10    100\n",
       "12    100\n",
       "27    100\n",
       "16    100\n",
       "1     100\n",
       "25    100\n",
       "26    100\n",
       "21    100\n",
       "23    100\n",
       "3      84\n",
       "22     82\n",
       "9      82\n",
       "24     62\n",
       "4      62\n",
       "2      51\n",
       "11     49\n",
       "19     45\n",
       "6      41\n",
       "8      27\n",
       "13     24\n",
       "20      8\n",
       "15      7\n",
       "5       2\n",
       "14      2\n",
       "Name: event_id, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.event_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выяснилось, корпус также содержит дубликаты некоторый новостей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>9</td>\n",
       "      <td>Коммерсантъ. Новости информ. центра, Москва, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     event_id                                               text\n",
       "513         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "518         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "522         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "526         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "528         9   Коммерсантъ. Новости информ. центра, Москва, ...\n",
       "536         9   Коммерсантъ. Новости информ. центра, Москва, ..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news[df_news.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что размер корпуса без дубликатов составляет 1924 новости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1924"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news = df_news.drop_duplicates()\n",
    "len(df_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка текста\n",
    "\n",
    "Мы решили рассмотреть три варианта лемматизации, поскольку не все способны одинаково хорошо справляться с контекстной омонимией (как, например, во фразе \"Поля в поле моет пол и заполнила пол поля\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_with_pymorphy(tokens):\n",
    "    return [morph.parse(word)[0].normal_form for word in tokens]\n",
    "\n",
    "def normalize_with_mystem(tokens):\n",
    "    return ''.join(m.lemmatize(' '.join(tokens))).split()\n",
    "\n",
    "def normalize_with_texterra(tokens):\n",
    "    time.sleep(10)\n",
    "    text = ' '.join(tokens)\n",
    "    return [token[3] for token in list(t.lemmatization(text))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pymorphy2: 0.90 (поль в пол мыть половина и заполнить половина поль)\n",
      "MyStem: 0.87 (поле в поле мыть пол и заполнять пол поля)\n",
      "Texterra: 0.88 (поле в поле мывать пол и заполнять пол поле)\n"
     ]
    }
   ],
   "source": [
    "tricky_text = 'Поля в поле моет пол и заполнила пол поля'\n",
    "gold_standard_normalized = 'поля в поле мыть пол и заполнить половина поле'\n",
    "\n",
    "for normalizer, normalize in [('Pymorphy2', normalize_with_pymorphy),\n",
    "                              ('MyStem', normalize_with_mystem),\n",
    "                              ('Texterra', normalize_with_texterra)]:\n",
    "    normalized_text = ' '.join(normalize((alpha_tokenizer.tokenize(tricky_text))))\n",
    "    print('{}: {:0.2f} ({})'.format(normalizer, SequenceMatcher(None, gold_standard_normalized, normalized_text).ratio(), normalized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы представим лемматизированные данные в виде трёх наборов данных и проверим, на каком из них рассмотренные алгоритмы покажут наилучшие результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_type_of_tokens = {}\n",
    "tokenizers = {'pymorphy': normalize_with_pymorphy, 'texterra': normalize_with_texterra, 'mystem': normalize_with_mystem}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные предварительно сериализованы в .pickle-файлы для того, чтобы впоследствии к ним можно было удобно обращаться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, tokenizer in tokenizers.items():\n",
    "    all_texts = []\n",
    "    for text in texts:\n",
    "        # print(texts.index(text)) Может, не надо их принтовать?\n",
    "        text = sub(r'http\\S+', '', text)\n",
    "        tokens = alpha_tokenizer.tokenize(text)\n",
    "        tokens = tokenizer(tokens)\n",
    "        all_texts.append(tokens)\n",
    "    # print(all_texts) Может, не надо их принтовать?\n",
    "    with open('tokens_from_' + name + '.pickle', 'wb') as f:\n",
    "        pickle.dump(all_texts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "for filename in glob.iglob(path.join(path.dirname(__file__),'lemmas_from_*.pickle'), recursive=True):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data_lemmas = pickle.load(f)\n",
    "        data[path.splitext(path.basename(filename))[0]] = data_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация\n",
    "\n",
    "Далее для каждого типа лемм создаем W2V модели, матрицу векторов для всех текстов, сохраняем ее в pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for vectorizer, vectorizer_name in [(TfidfVectorizer(ngram_range=ngram_range), 'TF-IDF Vectorizer'),\n",
    "                          (CountVectorizer(ngram_range=ngram_range), 'Count Vectorizer'), \n",
    "                          (HashingVectorizer(ngram_range=ngram_range), 'Hashing Vectorizer')]:\n",
    "    count_model = vectorizer\n",
    "    X = count_model.fit_transform(words_list)\n",
    "    normalizer = Normalizer()\n",
    "    U = normalizer.fit_transform(X)\n",
    "    for clusterizer, clusterizer_name in [(SpectralClustering(n_clusters=n_clusters), 'Spectral Clustering'),\n",
    "                                         (KMeans(n_clusters=n_clusters), 'KMeans'),\n",
    "                                         (AgglomerativeClustering(n_clusters=n_clusters), 'AgglomerativeClustering')]:\n",
    "        spectral = clusterizer.fit(U.toarray())\n",
    "        print('Vectroizer: {}, Clusterizer: {} \\nARI = {:0.2f}, V = {:0.2f}, completenss = {:0.2f}, homogenity = {:0.2f} \\n======'.format(\n",
    "                    vectorizer_name,\n",
    "                    clusterizer_name,\n",
    "                    adjusted_rand_score(spectral.labels_, df_news.event_id),\n",
    "                    completeness_score(spectral.labels_, df_news.event_id),\n",
    "                    v_measure_score(spectral.labels_, df_news.event_id),\n",
    "                    homogeneity_score(spectral.labels_, df_news.event_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_features = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name_of_alg, list_of_docs in data.items():\n",
    "    \n",
    "    model = gensim.models.Word2Vec(list_of_docs, size=num_of_features, min_count=30, window=30)\n",
    "    #model.save(name_of_alg.replace('lemmas','w2v_model2')+'.mdl')\n",
    "    \n",
    "    vectors_list = []\n",
    "    \n",
    "    for text_id in range(len(list_of_docs)):\n",
    "        vector_for_each_text = []\n",
    "        \n",
    "        for word in list_of_docs[text_id]:\n",
    "            try:\n",
    "                featureVec = np.zeros(shape=(1, num_of_features), dtype='float32')\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "                vector_for_each_text.append(featureVec)\n",
    "                \n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        first_vector = np.array(vector_for_each_text[0])\n",
    "        for i in range(1, len(vector_for_each_text)):\n",
    "            first_vector = np.add(first_vector, vector_for_each_text[i])\n",
    "            \n",
    "        resultVec = np.divide(first_vector, len(vector_for_each_text))\n",
    "        vectors_list.append(resultVec)\n",
    "\n",
    "    vectors_array = np.array(vectors_list[0])\n",
    "    for i in range(1, len(vectors_list)):\n",
    "        vectors_array = np.vstack((vectors_array,vectors_list[i]))\n",
    "\n",
    "    with open(name_of_alg.replace('lemmas', 'vectors2')+ '.pickle', 'wb') as f:\n",
    "            pickle.dump(np.matrix(vectors_array), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое происходит с Doc2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name_of_alg, list_of_docs in data.items():\n",
    "    \n",
    "    #тут короче преобразования для того, чтобы doc2vec нормально кушал тексты \n",
    "    sentences = [gensim.models.doc2vec.TaggedDocument(words=list_of_docs[doc], tags=[u'text']) for doc \n",
    "                 in range(len(list_of_docs))]\n",
    "    model = gensim.models.doc2vec.Doc2Vec(sentences, size=num_of_features, min_count=70, window=70)\n",
    "    #model.save(name_of_alg.replace('lemmas', 'd2v_model') + '.mdl')\n",
    "    model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "\n",
    "    vectors_list = []\n",
    "    for text_id in range(len(list_of_docs)):\n",
    "        vector_for_each_text = []\n",
    "        for word in list_of_docs[text_id]:\n",
    "            try:\n",
    "                featureVec = np.zeros(shape=(1, num_of_features), dtype='float32')\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "                vector_for_each_text.append(featureVec)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        first = np.array(vector_for_each_text[0])\n",
    "\n",
    "        for i in range(1, len(vector_for_each_text)):\n",
    "            first = np.add(first, vector_for_each_text[i])\n",
    "        resultVec = np.divide(first, len(vector_for_each_text))\n",
    "        vectors_list.append(resultVec)\n",
    "\n",
    "    vectors_array = np.array(vectors_list[0])\n",
    "\n",
    "    for i in range(1, len(vectors_list)):\n",
    "        vectors_array = np.vstack((vectors_array, vectors_list[i]))\n",
    "        \n",
    "    with open(name_of_alg.replace('lemmas', 'vectors_d2v') + '.pickle', 'wb') as f:\n",
    "            pickle.dump(np.matrix(vectors_array), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация\n",
    "\n",
    "Мы используем три алгоритма кластеризации, представленных в библиотеке ```scikit-learn```.\n",
    "\n",
    "**Метод k-means** — метод кластеризации, основанный на минимизации суммарного квадратичного отклонения точек кластеров от их центроидов:\n",
    "${\\displaystyle V=\\sum _{i=1}^{k}\\sum _{x_{j}\\in S_{i}}(x_{j}-\\mu _{i})^{2}} V=\\sum _{i=1}^{k}\\sum _{x_{j}\\in S_{i}}(x_{j}-\\mu _{i})^{2}$\n",
    "где $k$ — число кластеров, $S_{i}$ — полученные кластеры, $i=1,2,\\dots ,k$ и $\\mu _{i}$ — центры масс векторов $x_{j}\\in S_{i}$.\n",
    "\n",
    "**Метод Agglomerative Clustering** -- иерархический метод кластеризации, работающий \"снизу вверх\". В нём каждый объект из выборки получает собственный кластер, и далее пары кластеров итеративно объединяются в соответствии с заданными условиями, пока не получится желаемое число кластеров.\n",
    "\n",
    "**Метод Ши-Малика (Spectral Clustering)** -- метод кластеризации, который использует спектр (собственные значения) матрицы подобия данных (количественной оценки относительного сходства каждой пары точек в наборе данных) меньшей размерности. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_alg = [KMeans, AgglomerativeClustering, SpectralClustering]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для отрисовки Agglomerative clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotClusters(a):\n",
    "    z = hac.linkage(a, method='ward')\n",
    "    hac.dendrogram(z)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки используем \"настоящие\" метки новостей из ```raw_news.csv```. Чтобы проверить качество работы алгоритмов на кластеры, будем использовать следующие метрики:\n",
    "\n",
    "* **Adjusted Rand Index** ((RI - Expected_RI) / (max(RI) - Expected_RI)\n",
    "* **Homogenity** -- результат кластеризации удовлетворяет требованию homogenity, если все его кластеры содержат только точки данных, которые являются членами одного класса.\n",
    "* **Completeness** -- результат кластеризации удовлетворяет требованию completeness, если все точки данных, являющиеся членами данного класса, являются элементами одного и того же кластера.\n",
    "* **V-score** (среднее гармоническое между homogeneity и completeness: V = 2 * (homogeneity * completeness) / (homogeneity + completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = list(df_news.event_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь для каждого типа лемм применяем PipeLine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in glob.iglob('vectors_d2v_from_*.pickle', recursive=True):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data_model = pickle.load(f)\n",
    "        \n",
    "        for alg in list_of_alg:\n",
    "            print(alg, filename.replace('vectors_d2v_from_', '').replace('.pickle',''))\n",
    "            \n",
    "            pipeline = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                                 ('svd', TruncatedSVD(n_components=150)),\n",
    "                                 ('norm', Normalizer()),\n",
    "                                 ('clust', alg(n_clusters=28))\n",
    "                                ])\n",
    "            pipeline.fit(data_model)\n",
    "            \n",
    "            explained_variance = pipeline.named_steps['svd'].explained_variance_ratio_.sum()\n",
    "            print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "                        \n",
    "            clust_labels = pipeline.named_steps['clust'].labels_\n",
    "\n",
    "            print(\"Homogeneity:\", homogeneity_score(labels, clust_labels))\n",
    "            print(\"Completeness:\", completeness_score(labels, clust_labels))\n",
    "            print(\"V-measure\",  v_measure_score(labels, clust_labels))\n",
    "            print(\"Adjusted Rand-Index:\",  adjusted_rand_score(labels, clust_labels))\n",
    "            print(confusion_matrix(labels, clust_labels))\n",
    "            print('==============')\n",
    "        \n",
    "        if alg == AgglomerativeClustering:\n",
    "            plotClusters(data_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот тут вот сверху confusion matrix, если мы будем рисовать ее..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обсуждение\n",
    "\n",
    "Полученные результаты говорят нам о том, что в большинстве случаев полученные с помощью алгоритмов метки кластеров были похожи на метки кластеров, проставленные людьми; при этом пока ничего не понятно о том, действительно ли полученные кластеры будут интерпретируемы. Для этого мы решеили попробовать разбить данные на 5 кластеров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tokens_from_pymorphy.pickle', 'rb') as f:\n",
    "    tokens = pickle.load(f)\n",
    "words_list = [' '.join([word for word in sentence]) for sentence in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(words_list)\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "U = svd.fit_transform(X) \n",
    "clusterizer = SpectralClustering(n_clusters=5)\n",
    "clusterizer_model = clusterizer.fit(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = clusterizer_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events_merged = []\n",
    "\n",
    "for event in range(5):\n",
    "    events = set([df_news.event_id.values[i] for i, x in enumerate(labels) if x == event])\n",
    "    events_merged.append(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дональд Трамп вступил в должность президента США.\n",
      "Вышел фильм Навального «он Вам не димон»\n",
      "Умер Дэвид рокфеллер\n",
      "теракт произошел в центре Лондона\n",
      "Юлию Самойлову не пустили на евровидении в Киеве\n",
      "В центре Киева был убит бывший депутат Госдумы РФ от КПРФ Денис Вороненков\n",
      "Митинг в москве против коррупции\n",
      "SpaceX впервые в истории запустила и посадила уже летавшую ракету-носитель\n",
      "Умер Евгений Евтушенко\n",
      "Премьер Медведев выступает перед депутатами Госдумы с отчетом об итогах работы правительства за 2016 год\n",
      "Несанкционированные акции в Москве апрель\n",
      "Чемпионат мира по хоккею\n",
      "Победа Макрона во Франции\n",
      "Митинг против Реновации в Москве\n",
      "Ураган в Москве\n",
      "Парламентские выборы в Великобритании\n",
      "Горячая линия Президента Путина\n",
      "Кубок конфедерации FiFA\n",
      "Теракт в Барселоне\n",
      "Единый день голосования\n",
      "=======\n",
      "Дональд Трамп вступил в должность президента США.\n",
      "CNN показала фильм «Владимир Путин — самый влиятельный человек в мире».\n",
      "Юлию Самойлову не пустили на евровидении в Киеве\n",
      "В центре Киева был убит бывший депутат Госдумы РФ от КПРФ Денис Вороненков\n",
      "Тиллерсон посещает Москву и встречается с Путиным\n",
      "Несанкционированные акции в Москве апрель\n",
      "Путин и Меркель в Сочи\n",
      "Победа Макрона во Франции\n",
      "Парламентские выборы в Великобритании\n",
      "Горячая линия Президента Путина\n",
      "Саммит G20\n",
      "=======\n",
      "Правительство внесло в Госдуму законопроект о курортных сборах\n",
      "=======\n",
      "Скоропостижно скончался постпред России при ООН Виталий Чуркин.\n",
      "Умер Евгений Евтушенко\n",
      "Умер Дэвид рокфеллер\n",
      "=======\n",
      "Власти Петербурга согласились передать РПЦ Исаакиевский собор.\n",
      "Вышел фильм Навального «он Вам не димон»\n",
      "Митинг в москве против коррупции\n",
      "Несанкционированные акции в Москве апрель\n",
      "Митинг против Реновации в Москве\n",
      "Акции протеста 12 июня\n",
      "Кубок конфедерации FiFA\n",
      "=======\n"
     ]
    }
   ],
   "source": [
    "for events_set in events_merged:\n",
    "    for event in events_set:\n",
    "        print(df_events.loc[df_events.index == event]['name'].values[0])\n",
    "    print('=======')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Вот так вот"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение\n",
    "\n",
    "Таким образом,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
